
A = 所有可能動作
Q = 總獎勵網路
(狀態, 動作) -> Q -> 預測之後可得的總獎勵

t時刻之後可得的總獎勵 = t時刻的當下獎勵 + 衰減率 * t+1時刻之後可得的總獎勵

環境 := 觀察()
舊狀態.push(環境)
for time = 1 to T
{
  if (要隨機動作嗎?())
    動作 := 隨機動作()
  else
    動作 := (a, 使得 Q(舊狀態, a in A)有最大值)
  做(動作)
  
  # 遊戲畫面變了
  #update新東西
  新環境 := 觀察()
  新狀態 := 舊狀態
  新狀態.push(動作).push(新環境)
  
  當下獎勵 := 計算當下獎勵(舊環境, 新環境)
  紀錄表.push(舊狀態, 動作, 新狀態, 當下獎勵)
  
  # update舊東西
  舊狀態 := 新狀態
  舊環境 := 新環境
  
  /* 以下用紀錄表來更新神經網路 */
  隨機紀錄 := 紀錄表.隨機抽一個()
  
  更準確的_之後可得總獎勵 := 隨機紀錄->當下獎勵 + max(Q(隨機紀錄->新狀態, a in A))

  不那麼準確的_之後可得總獎勵 := Q(隨機紀錄->舊狀態, 隨機紀錄->動作)
  
  獎勵網路.train(更準確的, 不那麼準確的) #optimizer = sgd, loss = mse
}
